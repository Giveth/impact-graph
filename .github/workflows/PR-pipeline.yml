name: run tests on pull request

on:
  pull_request:
    branches:
      - master
      - staging

# Performance: Cancel outdated workflow runs
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

# Security: Set minimal default permissions for all jobs
permissions:
  contents: read  # All jobs can read code, but can't write by default

jobs:
  lint-and-build:
    name: Lint and Build
    runs-on: ubuntu-latest
    permissions:
      contents: read        # Read code
      pull-requests: write  # Post lint comments
      statuses: write      # Update check status

    steps:
      - name: Check out Git repository
        uses: actions/checkout@v5
        with:
          fetch-depth: 1

      - name: Set up Node.js
        uses: actions/setup-node@v6
        with:
          node-version: 22.18.0
          cache: 'npm'

      - name: Install Node.js dependencies
        run: npm ci

      - name: Run linters
        uses: wearerequired/lint-action@v2
        with:
          eslint: true
          #prettier: true
          continue_on_error: true

      - name: Build project
        run: npm run build

  test:
    name: Integration Tests - ${{ matrix.test-group-name }}
    runs-on: ubuntu-latest
    needs: lint-and-build
    
    # Performance: Run test groups in parallel
    strategy:
      fail-fast: false  # Don't cancel other groups if one fails
      matrix:
        include:
          - test-group-name: "Resolvers"
            test-pattern: "src/resolvers/**/*.test.ts"
          - test-group-name: "Repositories"
            test-pattern: "src/repositories/**/*.test.ts"
          - test-group-name: "Services"
            test-pattern: "src/services/**/*.test.ts"
          - test-group-name: "Server & Routes"
            test-pattern: "src/server/**/*.test.ts src/routers/**/*.test.ts"
          - test-group-name: "Entities & Utils"
            test-pattern: "src/entities/**/*.test.ts src/utils/**/*.test.ts src/adapters/**/*.test.ts"
          - test-group-name: "Workers & Migrations"
            test-pattern: "src/workers/**/*.test.ts migration/tests/*.test.ts"
    
    # Security: Explicit minimal permissions
    permissions:
      contents: read        # Read code
      pull-requests: write  # Comment test results
      checks: write        # Update check runs
    
    services:
      # Label used to access the service container
      redis:
        # Docker Hub image - Security: Consider using specific version tag
        image: redis:7.4-alpine  # Performance: Alpine is lighter, Security: specific version
        # Set health checks to wait until redis has started
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 6379:6379
      postgres:
        image: ghcr.io/giveth/postgres-givethio:latest
        env:
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
          POSTGRES_DB: givethio
          PGDATA: /var/lib/postgresql/data/pgdata
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5443:5432

    steps:
      - name: Checkout
        uses: actions/checkout@v5
        with:
          fetch-depth: 1
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v5.1.0
        with:
          aws-access-key-id: ${{ secrets.AWS_S3_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_S3_SECRET_ACCESS_KEY }}
          aws-region: ${{ secrets.AWS_S3_REGION }}
          mask-aws-account-id: true  # Security: Hide account ID in logs

      # Performance: Get current week for cache key (caches DB backup for the week)
      - name: Get current week
        id: date
        run: |
          # Get ISO week number (e.g., 2025-W44)
          WEEK=$(date +'%Y-W%V')
          echo "week=$WEEK" >> $GITHUB_OUTPUT
          echo "Cache key will be: db-backup-staging-$WEEK"

      # Performance: Cache database backup to avoid repeated downloads
      # Cache is shared across all runs in the same week (7 days)
      - name: Cache database backup
        id: cache-db
        uses: actions/cache@v4
        with:
          path: /tmp/db_backup.zip
          # Cache key: week-based so all runs same week share cache
          # Format: db-backup-staging-2025-W44 (changes every Monday)
          key: db-backup-staging-${{ steps.date.outputs.week }}
          restore-keys: |
            db-backup-staging-

      # Only download if cache miss
      - name: Download latest DB backup from S3
        if: steps.cache-db.outputs.cache-hit != 'true'
        run: |
          set -e
          echo "üì• Cache miss - downloading fresh backup from S3..."
          FILENAME=$(aws s3 ls ${{ secrets.AWS_S3_BUCKET_PATH_STAGING }}/ | sort | tail -n 1 | awk '{print $4}')
          if [ -z "$FILENAME" ]; then
            echo "Error: No backup file found in S3"
            exit 1
          fi
          echo "Downloading backup: $FILENAME"
          aws s3 cp ${{ secrets.AWS_S3_BUCKET_PATH_STAGING }}/$FILENAME /tmp/db_backup.zip
          echo "‚úÖ Download complete: $(du -h /tmp/db_backup.zip | cut -f1)"

      - name: Verify cached backup exists
        if: steps.cache-db.outputs.cache-hit == 'true'
        run: |
          echo "‚úÖ Cache hit! Using cached database backup"
          echo "Cached file size: $(du -h /tmp/db_backup.zip | cut -f1)"
          ls -lh /tmp/db_backup.zip

      - name: Unzip and validate DB backup
        run: |
          set -e
          echo "üì¶ Extracting database backup..."
          unzip -q /tmp/db_backup.zip -d /tmp
          
          # Security: Validate the backup file exists and is readable
          SQL_FILE=$(find /tmp/backups/givethio-staging/*.sql -type f -print -quit 2>/dev/null)
          if [ ! -f "$SQL_FILE" ]; then
            echo "Error: SQL backup file not found"
            exit 1
          fi
          
          mv "$SQL_FILE" /tmp/backups/givethio-staging/db_backup.sql
          echo "‚úÖ Database backup prepared: $(du -h /tmp/backups/givethio-staging/db_backup.sql | cut -f1)"

      - name: Wait for PostgreSQL to become ready
        run: |
          for i in {1..10}
          do
            pg_isready -h localhost -p 5443 -U postgres && echo Success && break
            echo -n .
            sleep 1
          done

      - name: Use Node.js
        uses: actions/setup-node@v6
        with:
          node-version: 22.18.0
          cache: 'npm'

      - name: Install dependencies
        run: npm ci

      # Performance: Cache compiled TypeScript for faster reruns
      - name: Cache TypeScript build
        uses: actions/cache@v4
        with:
          path: |
            .tsbuildinfo
            dist/
          key: ts-build-${{ hashFiles('src/**/*.ts', 'tsconfig.json') }}
          restore-keys: |
            ts-build-

      # Performance: Check if we have a cached post-migration database state FIRST
      - name: Get migration checksum
        id: migration-checksum
        run: |
          # Create checksum of all migration files to detect changes
          CHECKSUM=$(find migration -name "*.ts" -type f -exec sha256sum {} \; | sort | sha256sum | cut -d' ' -f1)
          echo "checksum=$CHECKSUM" >> $GITHUB_OUTPUT
          echo "Migration checksum: $CHECKSUM"

      - name: Cache post-migration database state
        id: cache-migrated-db
        uses: actions/cache@1bd1e32a3bdc45362d1e726936510720a7c30a57  # v4.2.0
        with:
          path: /tmp/migrated_db_dump.sql
          key: migrated-db-${{ steps.date.outputs.week }}-${{ steps.migration-checksum.outputs.checksum }}
          restore-keys: |
            migrated-db-${{ steps.date.outputs.week }}-

      # Only restore initial backup if we DON'T have cached migrated DB
      - name: Restore initial DB backup
        if: steps.cache-migrated-db.outputs.cache-hit != 'true'
        run: |
          echo "üì¶ Restoring initial database backup (will run migrations after)..."
          PGPASSWORD=postgres psql -h localhost -p 5443 -U postgres -d givethio \
            < /tmp/backups/givethio-staging/db_backup.sql
          echo "‚úÖ Initial database restored"

      # Restore from cached migrated database if available (SKIP initial restore)
      - name: Restore cached post-migration database
        if: steps.cache-migrated-db.outputs.cache-hit == 'true'
        run: |
          echo "‚ö° Restoring from cached post-migration database..."
          echo "This database already has migrations applied, skipping initial backup!"
          PGPASSWORD=postgres psql -h localhost -p 5443 -U postgres -d givethio \
            --set ON_ERROR_STOP=on < /tmp/migrated_db_dump.sql
          echo "‚úÖ Cached database restored (skipped migrations!)"

      # Only run migrations if no cached migrated DB
      - name: Run migrations
        if: steps.cache-migrated-db.outputs.cache-hit != 'true'
        run: |
          echo "üîÑ Running migrations (no cache)..."
          npm run db:migrate:run:test
          echo "‚úÖ Migrations complete"

      # Create dump of migrated database for caching
      - name: Create post-migration database dump
        if: steps.cache-migrated-db.outputs.cache-hit != 'true'
        run: |
          echo "üíæ Creating post-migration database dump for caching..."
          PGPASSWORD=postgres pg_dump -h localhost -p 5443 -U postgres -d givethio \
            --no-owner --no-acl > /tmp/migrated_db_dump.sql
          
          # Verify dump was created and show size
          if [ ! -f /tmp/migrated_db_dump.sql ]; then
            echo "‚ùå ERROR: Dump file was not created!"
            exit 1
          fi
          
          DUMP_SIZE=$(du -h /tmp/migrated_db_dump.sql | cut -f1)
          DUMP_SIZE_MB=$(du -m /tmp/migrated_db_dump.sql | cut -f1)
          echo "‚úÖ Dump created: $DUMP_SIZE ($DUMP_SIZE_MB MB)"
          
          # Warn if dump is very large
          if [ $DUMP_SIZE_MB -gt 2000 ]; then
            echo "‚ö†Ô∏è  Warning: Dump is large (${DUMP_SIZE_MB}MB), caching may be slow"
          fi

      # Debug: Verify cache file exists before tests
      - name: Verify cache file for saving
        if: steps.cache-migrated-db.outputs.cache-hit != 'true'
        run: |
          echo "üîç Verifying dump file exists for caching..."
          if [ -f /tmp/migrated_db_dump.sql ]; then
            ls -lh /tmp/migrated_db_dump.sql
            echo "‚úÖ File exists and will be cached after job completes"
          else
            echo "‚ùå ERROR: Dump file not found! Cache will not be saved!"
            exit 1
          fi

      # Performance: Run tests with optimized Node.js settings
      - name: Run tests - ${{ matrix.test-group-name }}
        run: |
          echo "üß™ Running test group: ${{ matrix.test-group-name }}"
          echo "üìÅ Test pattern: ${{ matrix.test-pattern }}"
          NODE_ENV=test npx mocha \
            --require ts-node/register \
            --timeout 90000 \
            --exit \
            --retries 2 \
            ./test/pre-test-scripts.ts \
            ${{ matrix.test-pattern }}
        env:
          # Performance: Optimize Node.js memory for testing
          NODE_OPTIONS: "--max-old-space-size=4096"
          ETHERSCAN_API_KEY: ${{ secrets.ETHERSCAN_API_KEY }}
          XDAI_NODE_HTTP_URL: ${{ secrets.XDAI_NODE_HTTP_URL }}
          INFURA_API_KEY: ${{ secrets.INFURA_API_KEY }}
          INFURA_ID: ${{ secrets.INFURA_ID }}
          POLYGON_SCAN_API_KEY: ${{ secrets.POLYGON_SCAN_API_KEY }}
          OPTIMISTIC_SCAN_API_KEY: ${{ secrets.OPTIMISTIC_SCAN_API_KEY }}
          CELO_SCAN_API_KEY: ${{ secrets.CELO_SCAN_API_KEY }}
          CELO_ALFAJORES_SCAN_API_KEY: ${{ secrets.CELO_ALFAJORES_SCAN_API_KEY }}
          ARBITRUM_SCAN_API_KEY: ${{ secrets.ARBITRUM_SCAN_API_KEY }}
          ARBITRUM_SEPOLIA_SCAN_API_KEY: ${{ secrets.ARBITRUM_SEPOLIA_SCAN_API_KEY }}
          BASE_SCAN_API_KEY: ${{ secrets.BASE_SCAN_API_KEY }}
          BASE_SEPOLIA_SCAN_API_KEY: ${{ secrets.BASE_SEPOLIA_SCAN_API_KEY }}
          ZKEVM_MAINNET_SCAN_API_KEY: ${{ secrets.ZKEVM_MAINNET_SCAN_API_KEY }}
          ZKEVM_CARDONA_SCAN_API_KEY: ${{ secrets.ZKEVM_CARDONA_SCAN_API_KEY }}
          MORDOR_ETC_TESTNET: ${{ secrets.MORDOR_ETC_TESTNET }}
          ETC_NODE_HTTP_URL: ${{ secrets.ETC_NODE_HTTP_URL }}
          DROP_DATABASE: ${{ secrets.DROP_DATABASE_DURING_TEST_STAGING }}
          SOLANA_TEST_NODE_RPC_URL: ${{ secrets.SOLANA_TEST_NODE_RPC_URL }}
          SOLANA_DEVNET_NODE_RPC_URL: ${{ secrets.SOLANA_DEVNET_NODE_RPC_URL }}
          SOLANA_MAINNET_NODE_RPC_URL: ${{ secrets.SOLANA_MAINNET_NODE_RPC_URL }}
          MPETH_GRAPHQL_PRICES_URL: ${{ secrets.MPETH_GRAPHQL_PRICES_URL }}
          GIV_POWER_SUBGRAPH_URL: ${{ secrets.GIV_POWER_SUBGRAPH_URL }}
          VERIFY_RIGHT_URL: ${{ secrets.VERIFY_RIGHT_URL }}
          VERIFY_RIGHT_TOKEN: ${{ secrets.VERIFY_RIGHT_TOKEN }}
